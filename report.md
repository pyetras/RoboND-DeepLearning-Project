# Follow Me project report

## Model evaluation
```
print(final_score)
0.41524076323668896
```

The weights are in `data/weights`.
I had to convert the project to tensorflow 1.5.1 for it to work with CUDA 9, I hope that the model formats did not change across the versions.
## Network architecture

| layer name | description                                  |
|------------|----------------------------------------------|
| input      | 160x160x3 input layer                        |
| layer_2    | 3 - Convolution x 64, stride 2 + batch norm  |
| layer_4    | 3 - Convolution x 128, stride 2 + batch norm |
| layer_8a   | 3 - Convolution x 256, stride 2 + batch norm |
| layer_8    | 3 - Conv x 256, stride 1 + batch norm        |
| layer_16a  | 3 - Conv x 512, stride 2 + batch norm        |
| layer_16   | 3 - Conv x 512 stride 1 + batch norm         |
| conv1      | 1x1 Conv x 256                               |
| layer8_up  | Decoder x 512                                |
| layer4_up  | Decoder x 256 + skip layer_4                 |
| layer2_up  | Decoder x 128 + skip layer_2                 |
| layer_up   | Decoder x 64                                 |
| output     | 3 - Conv x 3, softmax activation             |

I've modeled the encoder network after [VGG16](https://arxiv.org/abs/1409.1556) with some simplifications to reduce the training time.
Namely, max pool layers are replaced with convolution layers with stride = 2 and the number of stride 1 layers is lower than in the original representation.
I've found that adding some stride 1 layers removed the number of artifacts present in the output.
Decoder is a series of one bilinear upscaler + 2 convolution layers of the same size with stride 1.
Adding the convolution layers to the uspcaler improved the "silhouette" of detected objects and allowed the model to "learn" a better upscaler.
Additionally some decoder layers use a skip layer of higher resolution to allow for a finer resolution of the output.
Each convolution layer uses relu activation unless stated otherwise.
Each convolution layer except for the last is depth-separable to decrease the number of learned parameters.

The encoder layers are meant to learn to detect object based on their visual representation.
Their most useful output is the probability of each class of object being present in the image, without specifying where they are.
Image detection model could be trained to recognize for each pixel which class it belongs to, but this would require running a forward pass of the network for each pixel, which could be quite expensive.
The authors of the original [FCN paper](https://people.eecs.berkeley.edu/~jonlong/long_shelhamer_fcn.pdf) mention that all image detection models work reasonably well, so the above should also work with other popular network architectures such as AlexNet or GoogLeNet.

The 1x1 convolution layer serves as a spatial-aware replacement of the FC layers usually present at the end of an image detection network (encoder here).
FC layers and 1x1 layer here are useful to sum up (using appropriate activation function) and reduce the output of multiple constitutional filters.
Since the 1x1 layer remains a 4D tensor it preserves spatial information and can be used by the decoder.

Decoder layers take encoded spatial information generated by the encoder layers and learn how to best upsample the tensor, so that the output has the size and resolution of the input image.
The low resolution of lower upsampling layers would normally be carried over to the upper layers, resulting in a blurred image.
To combat that high resolution skip layers from the encoder network are concatenated with the upscaled layers, so that the convolutions can concatenate the high resolution shape information and low resolution semantic information.

## Hyperparameters
By manual fiddling I've found this set of parameters to work best for me:
```
learning_rate = 0.001
batch_size = 64
num_epochs = 15
steps_per_epoch = 200
validation_steps = 50
workers = 2
```
Decreasing the learning rate resulted in much slower convergence and did not improve the end result.

Batch size is the maximum that fit my GPU with the above network architecture.

Number of epochs and steps_per_epoch were chosen empirically - I've seen an improvement by increasing steps_per_epoch from 100 to 200 - it's possible that even further improvement is possible.

## Other applications and future improvement

The model would not work for different hero models out of the box, however pre-initializing the new model's layers with this model's weights should lead to
faster convergence.
The first few layers specialize in learning simple shapes that should stay similar and the architecture should work for similar kinds of heroes (by similar I mean without drastic changes to shape and size).

Further improvement could be achieved by pre-selecting more hero examples in the training set - currently there is an underrepresentation of hero compared to other classes.

Another idea would be to use a known image detection architecture for the encoder - or even initialize the network a pretrained detection model with the FC layers removed.
